{
  "hash": "fb14d013abd3b63ee2a6abfe3f924f31",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Incorporating Spatial Autocorrelation into Spatial Predictions\nformat:\n  html:\n    code-fold: false\n---\n\nSimilarly to example 1, we are using the meuse dataset again to perform a multi-target prediction of soil properties using a regression model. However, in this case we will attempt to account for spatial autocorrelation in the model directly by generating new features that are based on the distance-weighted means of surrounding spatial locations.\n\n::: {#0bcf6e26 .cell execution_count=1}\n``` {.python .cell-code}\nimport geopandas as gpd\nimport numpy as np\nfrom tempfile import NamedTemporaryFile\nfrom pyspatialml import Raster\nimport pyspatialml.datasets.meuse as ms\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n## Preparing the Raster Predictors\n\nImport the raster predictors from the `pyspatialml.datasets.meuse` module:\n\n::: {#ebe6123b .cell execution_count=2}\n``` {.python .cell-code}\npredictor_files = ms.predictors\ntraining_pts_file = ms.meuse\nstack = Raster(predictor_files)\nstack.drop('ffreq')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRaster Object Containing 11 Layers\n    attribute                                             values\n0       names  [chnl_dist, dem, dist, landimg2, landimg3, lan...\n1       files  [/Users/stevenpawley/GitHub/Pyspatialml/pyspat...\n2        rows                                                104\n3        cols                                                 78\n4         res                                       (40.0, 40.0)\n5  nodatavals  [-99999.0, -99999.0, -1.0, -1.0, -1.0, -1.0, -...\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n\n```\n:::\n:::\n\n\nIn order to generate new features from surrounding spatial locations, we need their x,y coordinates, which will will add to the stack of the raster predictors using the `pyspatialml.preprocessing.xy_coordinates` function:\n\n::: {#0393e2ac .cell execution_count=3}\n``` {.python .cell-code}\nfrom pyspatialml.preprocessing import xy_coordinates\n\nxy_layers = xy_coordinates(stack.iloc[0], NamedTemporaryFile(suffix=\".tif\").name)\nstack = stack.append(xy_layers, in_place=False)\n```\n:::\n\n\nQuickly plot the raster predictors:\n\n::: {#cba259ea .cell execution_count=4}\n``` {.python .cell-code}\nmpl.style.use('seaborn-v0_8')\naxs = stack.plot(figsize=(9, 7))\nax = axs.flatten()[10]\nim = ax.images\nim[0].colorbar.set_ticks([1,2,3])\nax = axs.flatten()[8]\nax.tick_params(axis='x', labelrotation=65)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](spatial-features_files/figure-html/cell-5-output-1.png){width=817 height=663}\n:::\n:::\n\n\n## Extract the Training Data\n\nSpatially query the raster predictors at the training point locations:\n\n::: {#06d01eb2 .cell execution_count=5}\n``` {.python .cell-code}\ntraining_pts = gpd.read_file(training_pts_file)\ntraining_df = stack.extract_vector(gdf=training_pts)\n\ntraining_df.index = training_df.index.get_level_values(\"geometry_idx\")\ntraining_df = training_df.merge(\n    training_pts.loc[:, (\"lead\", \"cadmium\", \"copper\", \"zinc\", \"om\")], \n    left_index=True, \n    right_index=True\n) \ntraining_df = training_df.dropna()\n```\n:::\n\n\nSplit the response/target variables from the predictors:\n\n::: {#3138a3c4 .cell execution_count=6}\n``` {.python .cell-code}\nX = training_df.loc[:, stack.names].values\ny = training_df.loc[:, ['lead', 'cadmium', 'copper', 'zinc', 'om']].values\n```\n:::\n\n\n## Develop a Spatially-Lagged Machine Learning Model\n\nAs well as using the ExtraTreeRegressor model which was also used in example 1, here we will use the custom `pyspatialml.estimators.SpatialLagRegressor` metalearner class to wrap the extratrees regressor into a model that adds a new feature based on the distance-weighted mean of spatially-proximal observations:\n\n::: {#e50ffc54 .cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom pyspatialml.transformers import KNNTransformer\nfrom sklearn.model_selection import cross_validate, KFold\nfrom sklearn.model_selection import GridSearchCV\n\n# define regressor\net = ExtraTreesRegressor(n_estimators=500, n_jobs=-1, random_state=1234)\n\nsoil_index = list(stack.names).index(\"soil\")\nxy_indexes = [list(stack.names).index(i) for i in [\"x_coordinates\", \"y_coordinates\"]]\n\npreproc = ColumnTransformer([\n    ('ohe', OneHotEncoder(categories='auto', handle_unknown='ignore'), [soil_index]),\n    ('lags', KNNTransformer(weights='distance', measure=\"mean\"), xy_indexes)\n], remainder='passthrough')\n\nwflow = Pipeline([\n    ('preproc', preproc),\n    ('regressor', et)\n])\n\nsearch_grid = {\"preproc__lags__n_neighbors\": [3, 5, 7, 9]}\ninner = KFold(n_splits=3, shuffle=True, random_state=1234)\nmodel = GridSearchCV(wflow, param_grid=search_grid, cv=inner, scoring=\"r2\")\n```\n:::\n\n\nFit the model and cross-validate:\n\n::: {#5d0230a7 .cell execution_count=8}\n``` {.python .cell-code}\nmodel = model.fit(X, y)\nmodel.best_params_\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n{'preproc__lags__n_neighbors': 9}\n```\n:::\n:::\n\n\n::: {#4a619dd2 .cell execution_count=9}\n``` {.python .cell-code}\nouter = KFold(n_splits=10, shuffle=True, random_state=1234)\n\nscores = cross_validate(model, X, y, scoring='neg_mean_squared_error', cv=outer, n_jobs=1)\nrmse = np.sqrt(-scores['test_score']).mean()\n\nprint(\"Our RMSE score is {}\".format(rmse))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOur RMSE score is 102.27495341202624\n```\n:::\n:::\n\n\nComparing the RMSE score the the score obtained in example 1, where the spatial structure of the training data was accounted for indirectly by added a variety of raster distance measures, we can see that the RMSE score is slightly improved.\n\n## Multi-Target Predictions\n\n::: {#ddc87c2a .cell execution_count=10}\n``` {.python .cell-code}\npreds = stack.predict(model)\npreds.rename(\n    {old: new for old, new in zip(preds.names, ['lead', 'cadmium', 'copper', 'zinc', 'om'])},\n    in_place=True\n)\npreds.lead.cmap = 'rainbow'\npreds.cadmium.cmap = 'rainbow'\npreds.copper.cmap = 'rainbow'\npreds.zinc.cmap = 'rainbow'\npreds.om.cmap = 'rainbow'\n```\n:::\n\n\n::: {#911ae7fb .cell execution_count=11}\n``` {.python .cell-code}\npreds.plot(out_shape=(200, 200), title_fontsize=14, figsize=(10, 8))\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](spatial-features_files/figure-html/cell-12-output-1.png){width=822 height=634}\n:::\n:::\n\n\n",
    "supporting": [
      "spatial-features_files"
    ],
    "filters": [],
    "includes": {}
  }
}